{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PytorchTutorial_CNN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFohzD97UktrrrLUmPiiCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ductt-1167/Pytorch-Tutorial/blob/main/PytorchTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9IioQez6L-h3"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from time import time \n",
        "import torchvision\n",
        "from torchvision import datasets, transforms \n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from torch import nn, optim"
      ],
      "metadata": {
        "id": "Z4YhoItSMzCo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Mnist dataset "
      ],
      "metadata": {
        "id": "RbtVjgEcWOfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataloader(num_workers=8,\n",
        "                       train_batch_size=64,\n",
        "                       eval_batch_size=64):\n",
        "\n",
        "    train_set = torchvision.datasets.MNIST(root=\"data\",\n",
        "                                             train=True,\n",
        "                                             download=True,\n",
        "                                             transform=transforms.ToTensor())\n",
        "    \n",
        "    val_set = torchvision.datasets.MNIST(root=\"data\",\n",
        "                                            train=False,\n",
        "                                            download=True,\n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                               batch_size=train_batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=num_workers)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
        "                                             batch_size=eval_batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=num_workers)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "gZmn5qE7Rnkn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader, valloader = prepare_dataloader()"
      ],
      "metadata": {
        "id": "65TL6dH9Wd2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ec9c99-e17b-4402-f285-b9f62a5916e4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot image data"
      ],
      "metadata": {
        "id": "AAvaGWp1VWbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataiter = iter(trainloader)\n",
        "# images, labels = dataiter.next()\n",
        "# print(type(images))\n",
        "# print(images.shape)\n",
        "# print(labels.shape)\n",
        "#\n",
        "# plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');\n",
        "#\n",
        "# figure = plt.figure()\n",
        "# num_of_images = 60\n",
        "# for index in range(1, num_of_images + 1):\n",
        "#     plt.subplot(6, 10, index)\n",
        "#     plt.axis('off')\n",
        "#     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
      ],
      "metadata": {
        "id": "e0pv52zNWk0Y"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the network\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html "
      ],
      "metadata": {
        "id": "kEooptpNW7Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNet(nn.Module):\n",
        "\tdef __init__(self, num_channels, classes):\n",
        "    # call the parrent constructor \n",
        "\t\tsuper(MyNet, self).__init__()\n",
        "\n",
        "    # Init layers: Conv, ReLU, Pool.\n",
        "\t\tself.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=20, kernel_size=(5, 5))\n",
        "\t\tself.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5))\n",
        "\n",
        "\t\tself.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\t\tself.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "\t\tself.fc1 = nn.Linear(in_features=800, out_features=500)\n",
        "\t\tself.fc2 = nn.Linear(in_features=500, out_features=classes)\n",
        "\n",
        "\t\tself.relu = nn.ReLU()\n",
        "\t\tself.log_soft = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t# pass the input through our first set of CONV => RELU => POOL layers\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = self.relu(x)\n",
        "\t\tx = self.pool1(x)\n",
        "\n",
        "\t\t# pass the output from the previous layer through the second set of CONV => RELU => POOL layers\n",
        "\t\tx = self.conv2(x)\n",
        "\t\tx = self.relu(x)\n",
        "\t\tx = self.pool2(x)\n",
        "\n",
        "\t\t# flatten the output from the previous layer and pass it through our only set of FC => RELU layers\n",
        "\t\tx = torch.flatten(x, 1)\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = self.relu(x)\n",
        "\n",
        "\t\t# pass the output to our softmax classifier to get our output predictions\n",
        "\t\tx = self.fc2(x)\n",
        "\t\toutput = self.log_soft(x)\n",
        "\n",
        "\t\t# return the output predictions\n",
        "\t\treturn output"
      ],
      "metadata": {
        "id": "ISbS-RyUW1-y"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training model"
      ],
      "metadata": {
        "id": "hwI9Ow5qK5HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the device we will be using to train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "6uXoHdO9LETN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training hyperparameters\n",
        "INIT_LR = 1e-3\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model = MyNet(num_channels=1, classes=len(trainloader.dataset.classes)).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=INIT_LR)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# calculate steps per epoch for training and validation set\n",
        "train_steps = len(trainloader.dataset) // BATCH_SIZE\n",
        "val_steps = len(valloader.dataset) // BATCH_SIZE\n"
      ],
      "metadata": {
        "id": "eopz3WqQIbYo"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a dictionary to store training history\n",
        "H = {\n",
        "\t\"train_loss\": [],\n",
        "\t\"train_acc\": [],\n",
        "\t\"val_loss\": [],\n",
        "\t\"val_acc\": []\n",
        "}\n",
        "\n",
        "# measure how long training is going to take\n",
        "print(\"[INFO] training the network...\")\n",
        "start_time = time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia8AIFMqLkMD",
        "outputId": "da0e1d75-3542-4c15-b24b-349328736bdc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training the network...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over our epochs\n",
        "for e in range(0, EPOCHS):\n",
        "\t# set the model in training mode\n",
        "\tmodel.train()\n",
        "\n",
        "\t# initialize the total training and validation loss\n",
        "\ttotal_train_loss = 0\n",
        "\ttotal_val_loss = 0\n",
        "\n",
        "\t# initialize the number of correct predictions in the training and validation step\n",
        "\ttrain_correct = 0\n",
        "\tval_correct = 0\n",
        "\n",
        "\t# loop over the training set\n",
        "\tfor i, (x, y) in enumerate(trainloader):\n",
        "\t\t# send the input to the device\n",
        "\t\t(x, y) = (x.to(device), y.to(device))\n",
        "\n",
        "\t\t# perform a forward pass and calculate the training loss\n",
        "\t\tpred = model(x)\n",
        "\t\tloss = loss_fn(pred, y)\n",
        "\n",
        "\t\t# zero out the gradients, perform the backpropagation step, and update the weights\n",
        "\t\topt.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\topt.step()\n",
        "\n",
        "\t\t# add the loss to the total training loss so far and calculate the number of correct predictions\n",
        "\t\ttotal_train_loss += loss\n",
        "\t\ttrain_correct += (pred.argmax(1) == y).type(\n",
        "\t\t\ttorch.float).sum().item()\n",
        "   \n",
        "  # switch off autograd for evaluation\n",
        "\twith torch.no_grad():\n",
        "\t\t# set the model in evaluation mode\n",
        "\t\tmodel.eval()\n",
        "\n",
        "\t\t# loop over the validation set\n",
        "\t\tfor i, (x, y) in enumerate(valloader):\n",
        "\t\t\t# send the input to the device\n",
        "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
        "\n",
        "\t\t\t# make the predictions and calculate the validation loss\n",
        "\t\t\tpred = model(x)\n",
        "\t\t\ttotal_val_loss += loss_fn(pred, y)\n",
        "\n",
        "\t\t\t# calculate the number of correct predictions\n",
        "\t\t\tval_correct += (pred.argmax(1) == y).type(\n",
        "\t\t\t\ttorch.float).sum().item()\n",
        "    \n",
        "  # calculate the average training and validation loss\n",
        "\tavg_train_loss = total_train_loss / train_steps\n",
        "\tavg_val_loss = total_val_loss / val_steps\n",
        "\n",
        "\t# calculate the training and validation accuracy\n",
        "\ttrain_correct = train_correct / len(trainloader.dataset)\n",
        "\tval_correct = val_correct / len(valloader.dataset)\n",
        "\n",
        "\t# update our training history\n",
        "\tH[\"train_loss\"].append(avg_train_loss.cpu().detach().numpy())\n",
        "\tH[\"train_acc\"].append(train_correct)\n",
        "\tH[\"val_loss\"].append(avg_val_loss.cpu().detach().numpy())\n",
        "\tH[\"val_acc\"].append(val_correct)\n",
        "\n",
        "\t# print the model training and validation information\n",
        "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
        "\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
        "\t\tavg_train_loss, train_correct))\n",
        "\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
        "\t\tavg_val_loss, val_correct))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh7cCYKoLlEZ",
        "outputId": "60386440-de5d-4471-a999-27b3ba87a674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] EPOCH: 1/10\n",
            "Train loss: 0.155060, Train accuracy: 0.9528\n",
            "Val loss: 0.041959, Val accuracy: 0.9867\n",
            "\n",
            "[INFO] EPOCH: 2/10\n",
            "Train loss: 0.046196, Train accuracy: 0.9855\n",
            "Val loss: 0.033349, Val accuracy: 0.9890\n",
            "\n",
            "[INFO] EPOCH: 3/10\n",
            "Train loss: 0.030136, Train accuracy: 0.9902\n",
            "Val loss: 0.024930, Val accuracy: 0.9919\n",
            "\n",
            "[INFO] EPOCH: 4/10\n",
            "Train loss: 0.022874, Train accuracy: 0.9927\n",
            "Val loss: 0.026712, Val accuracy: 0.9910\n",
            "\n",
            "[INFO] EPOCH: 5/10\n",
            "Train loss: 0.018057, Train accuracy: 0.9944\n",
            "Val loss: 0.024304, Val accuracy: 0.9908\n",
            "\n",
            "[INFO] EPOCH: 6/10\n",
            "Train loss: 0.013904, Train accuracy: 0.9955\n",
            "Val loss: 0.028141, Val accuracy: 0.9910\n",
            "\n",
            "[INFO] EPOCH: 7/10\n",
            "Train loss: 0.012330, Train accuracy: 0.9959\n",
            "Val loss: 0.030297, Val accuracy: 0.9897\n",
            "\n",
            "[INFO] EPOCH: 8/10\n",
            "Train loss: 0.009916, Train accuracy: 0.9967\n",
            "Val loss: 0.036942, Val accuracy: 0.9910\n",
            "\n",
            "[INFO] EPOCH: 9/10\n",
            "Train loss: 0.008654, Train accuracy: 0.9974\n",
            "Val loss: 0.029325, Val accuracy: 0.9917\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finish measuring how long training took\n",
        "end_time = time()\n",
        "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
        "\tend_time - start_time))"
      ],
      "metadata": {
        "id": "jln8wnpeQoa9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
